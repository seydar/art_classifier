<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <!-- Favicon -->
  <link href="../images/vl_blue.ico" type="image/x-icon" rel="icon"></link>
  <link href="../images/vl_blue.ico" type="image/x-icon" rel="shortcut icon"></link>
  <!-- Stylesheets -->
  <link href="../web.css" type="text/css" rel="stylesheet"></link>
  <link href="../pygmentize.css" type="text/css" rel="stylesheet"></link>
  <title>VLFeat - Documentation - C API</title>
  <link rel="stylesheet" type="text/css" href="../doxygen.css"></style>
  <!-- Scripts-->
  <!-- Google Custom Search -->
  <script xml:space="preserve">
    (function() {
    var cx = '003215582122030917471:oq23albfeam';
    var gcse = document.createElement('script'); gcse.type = 'text/javascript'; gcse.async = true;
    gcse.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') +
    '//www.google.com/cse/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(gcse, s);
    })();
  </script>
  <!-- Google Analytics -->
  <script xml:space="preserve" type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-4936091-2']);
    _gaq.push(['_trackPageview']);
    (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>
 </head>
 <!-- Body Start -->
 <body>
  <div id="header">
   <!-- Google CSE Search Box -->
   <div id="google" class="gcse-searchbox-only" data-resultsUrl="http://www.vlfeat.org/search.html"></div>
   <h1><a shape="rect" href="../index.html" class="plain"><span id="vlfeat">VLFeat</span><span id="dotorg">.org</span></a></h1>
  </div>
  <div id="headbanner">
   Documentation - C API
  </div>
  <div id="pagebody">
   <div id="sidebar"> <!-- Navigation Start -->
    <ul>
<li><a href="../index.html">Home</a>
</li>
<li><a href="../download.html">Download</a>
</li>
<li><a href="../doc.html">Documentation</a>
<ul>
<li><a href="../mdoc/mdoc.html">Matlab API</a>
</li>
<li><a href="index.html" class='active' >C API</a>
</li>
<li><a href="../man/man.html">Man pages</a>
</li>
</ul></li>
<li><a href="../overview/tut.html">Tutorials</a>
</li>
<li><a href="../applications/apps.html">Applications</a>
</li>
</ul>
   </div> <!-- sidebar -->
   <div id="content">
    <link rel="stylesheet" type="text/css" href="../doxygen.css"></style>
    <div class="doxygen">
<div>
<!-- Generated by Doxygen 1.8.1.1 -->
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li class="current"><a href="pages.html"><span>Related&#160;Pages</span></a></li>
      <li><a href="annotated.html"><span>Data&#160;Structures</span></a></li>
      <li><a href="files.html"><span>Files</span></a></li>
    </ul>
  </div>
</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">PEGASOS SVM solver </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><dl class="section author"><dt>Author:</dt><dd>Daniele Perrone </dd>
<dd>
Andrea Vedaldi</dd></dl>
<p><a class="el" href="pegasos_8h.html">pegasos.h</a> provides a basic implementation of the PEGASOS <a class="el" href="citelist.html#CITEREF_shalev-shwartz07pegasos">[12]</a> linear SVM solver.</p>
<ul>
<li><a class="el" href="pegasos.html#pegasos-overview">Overview</a><ul>
<li><a class="el" href="pegasos.html#pegasos-algorithm">Algorithm</a></li>
<li><a class="el" href="pegasos.html#pegasos-bias">Bias</a></li>
<li><a class="el" href="pegasos.html#pegasos-restarting">Restarting</a></li>
<li><a class="el" href="pegasos.html#pegasos-kernels">Non-linear kernels</a></li>
</ul>
</li>
</ul>
<h1><a class="anchor" id="pegasos-overview"></a>
Overview</h1>
<p>PEGASOS solves the <em>linear</em> SxVM learning problem</p>
<p class="formulaDsp">
<img class="formulaDsp" alt="\[ \min_{w} \frac{\lambda}{2} \|w\|^2 + \frac{1}{m} \sum_{i=1}^n \ell(w; (x_i,y_i)) \]" src="form_273.png"/>
</p>
<p>where <img class="formulaInl" alt="$ x_i $" src="form_112.png"/> are data vectors in <img class="formulaInl" alt="$ \mathbb{R}^d $" src="form_175.png"/>, <img class="formulaInl" alt="$ y_i \in \{-1,1\} $" src="form_274.png"/> are binary labels, <img class="formulaInl" alt="$ \lambda > 0 $" src="form_275.png"/> is the regularization parameter, and</p>
<p class="formulaDsp">
<img class="formulaDsp" alt="\[ \ell(w;(x_i,y_i)) = \max\{0, 1 - y_i \langle w,x_i\rangle\}. \]" src="form_276.png"/>
</p>
<p>is the <em>hinge loss</em>. The result of the optimization is a model <img class="formulaInl" alt="$ w \in \mathbb{R}^d $" src="form_277.png"/> that yields the decision function</p>
<p class="formulaDsp">
<img class="formulaDsp" alt="\[ F(x) = \operatorname{sign} \langle w, x\rangle. \]" src="form_278.png"/>
</p>
<p>It is well known that the hinge loss is a convex upper bound of the i01-loss of the decision function:</p>
<p class="formulaDsp">
<img class="formulaDsp" alt="\[ \ell(w;(x,y)) \geq \frac{1}{2}(1 - y F(x)). \]" src="form_279.png"/>
</p>
<p>PEGASOS is accessed by calling <a class="el" href="pegasos_8h.html#a63ecbe7cddbb96b261df82888008d622">vl_svmpegasos_train</a> or <a class="el" href="pegasos_8h.html#a2b9663fbc9d44b90efd7e75991845d3a">vl_svmpegasos_train_validation_data</a>.</p>
<h2><a class="anchor" id="pegasos-algorithm"></a>
Algorithm</h2>
<p>PEGASOS <a class="el" href="citelist.html#CITEREF_shalev-shwartz07pegasos">[12]</a> is a stochastic subgradient optimizer. At the <em>t</em>-th iteration the algorithm:</p>
<ul>
<li>Samples uniformly at random as subset <img class="formulaInl" alt="$ A_t$" src="form_280.png"/> of <em>k</em> of training pairs <img class="formulaInl" alt="$(x,y)$" src="form_281.png"/> from the <em>m</em> pairs provided for training (this subset is called mini batch).</li>
<li>Computes a subgradient <img class="formulaInl" alt="$ \nabla_t $" src="form_282.png"/> of the function <img class="formulaInl" alt="$ E_t(w) = \frac{1}{2}\|w\|^2 + \frac{1}{k} \sum_{(x,y) \in A_t} \ell(w;(x,y)) $" src="form_283.png"/> (this is the SVM objective function restricted to the minibatch).</li>
<li>Compute an intermediate weight vector <img class="formulaInl" alt="$ w_{t+1/2} $" src="form_284.png"/> by doing a step <img class="formulaInl" alt="$ w_{t+1/2} = w_t - \alpha_t \nalba_t $" src="form_285.png"/> with learning rate <img class="formulaInl" alt="$ \alpha_t = 1/(\eta t) $" src="form_286.png"/> along the subgradient. Note that the learning rate is inversely proportional to the iteration numeber.</li>
<li>Back projects the weight vector <img class="formulaInl" alt="$ w_{t+1/2} $" src="form_284.png"/> on the hypersphere of radius <img class="formulaInl" alt="$ \sqrt{\lambda} $" src="form_287.png"/> to obtain the next model estimate <img class="formulaInl" alt="$ w_{t+1} $" src="form_288.png"/>: <p class="formulaDsp">
<img class="formulaDsp" alt="\[ w_t = \min\{1, \sqrt{\lambda}/\|w\|\} w_{t+1/2}. \]" src="form_289.png"/>
</p>
 The hypersfere is guaranteed to contain the optimal weight vector <img class="formulaInl" alt="$ w^* $" src="form_290.png"/> <a class="el" href="citelist.html#CITEREF_shalev-shwartz07pegasos">[12]</a> .</li>
</ul>
<p>VLFeat implementation fixes to one the size of the mini batches <img class="formulaInl" alt="$ k $" src="form_178.png"/>.</p>
<h2><a class="anchor" id="pegasos-bias"></a>
Bias</h2>
<p>PEGASOS SVM formulation does not incorporate a bias. To learn an SVM with bias, the each data vector <img class="formulaInl" alt="$ x $" src="form_102.png"/> can be extended by a constant component <img class="formulaInl" alt="$ B $" src="form_291.png"/> (called <code>svm-&gt;biasMultiplier</code> in the code). In this case, the model <img class="formulaInl" alt="$ w $" src="form_292.png"/> has dimension <img class="formulaInl" alt="$ D + 1 $" src="form_293.png"/> and the SVM discriminat function is given by <img class="formulaInl" alt="$ F(x) = \operatorname{sign} (\langle w_{1:d}, x\rangle+ w_{d+1} B) $" src="form_294.png"/>. In our implementation the bias is kept in the variabl <code>svm-&gt;bias</code> and the model has dimension <img class="formulaInl" alt="$ D $" src="form_122.png"/>. If the bias multiplier <em>B</em> is large enough, the weight <img class="formulaInl" alt="$ w_{d+1} $" src="form_295.png"/> remains small and it has small contribution in the SVM regularization term <img class="formulaInl" alt="$ \| w \|^2 $" src="form_296.png"/>, better approximating the case of an SVM with bias. Unfortunately, setting the bias multiplier <img class="formulaInl" alt="$ B $" src="form_291.png"/> to a large value makes the optimization harder.</p>
<h2><a class="anchor" id="pegasos-restarting"></a>
Restarting</h2>
<p>VLFeat PEGASOS implementation can be restarted after any given number of iterations. This is useful to compute intermediate statistics or to load new data from disk for large datasets. It sufficient to use the Svm state object as input for the next run. Notice that a model learned using a "stopped-restarted" solver could slightly differ from one learned from a unique run of the solver. The difference could be more noticable if the user provides a permutation to decide the order of the data points visits.</p>
<h2><a class="anchor" id="pegasos-permutation"></a>
Permutation</h2>
<p>VLFeat PEGASOS can use a user-defined permutation to decide the order in which data points are visited (instead of using random sampling). By specifying a permutation the algorithm is guaranteed to visit each data point exactly once in each loop. The permutation needs not to be bijective. This can be used to visit certain data samples more or less often than others, implicitly reweighting their relative importance in the SVM objective function. This can be used to blanace the data.</p>
<h2><a class="anchor" id="pegasos-kernels"></a>
Non-linear kernels</h2>
<p>PEGASOS can be extended to non-linear kernels, but the algorithm is not particularly efficient in this setting [1]. When possible, it may be preferable to work with explicit feature maps.</p>
<p>Let <img class="formulaInl" alt="$ k(x,y) $" src="form_85.png"/> be a positive definite kernel. A <em>feature map</em> is a function <img class="formulaInl" alt="$ \Psi(x) $" src="form_101.png"/> such that <img class="formulaInl" alt="$ k(x,y) = \langle \Psi(x), \Psi(y) \rangle $" src="form_297.png"/>. Using this representation the non-linear SVM learning objective function writes:</p>
<p class="formulaDsp">
<img class="formulaDsp" alt="\[ \min_{w} \frac{\lambda}{2} \|w\|^2 + \frac{1}{m} \sum_{i=1}^n \ell(w; (\Psi(x)_i,y_i)). \]" src="form_298.png"/>
</p>
<p>Thus the only difference with the linear case is that the feature <img class="formulaInl" alt="$ \Psi(x) $" src="form_101.png"/> is used in place of the data <img class="formulaInl" alt="$ x $" src="form_102.png"/>.</p>
<p><img class="formulaInl" alt="$ \Psi(x) $" src="form_101.png"/> can be learned off-line, for instance by using the incomplete Cholesky decomposition <img class="formulaInl" alt="$ V^\top V $" src="form_299.png"/> of the Gram matrix <img class="formulaInl" alt="$ K = [k(x_i,x_j)] $" src="form_300.png"/> (in this case <img class="formulaInl" alt="$ \Psi(x_i) $" src="form_301.png"/> is the <em>i</em>-th columns of <em>V</em>). Alternatively, for additive kernels (e.g. intersection, Chi2) the explicit feature map computed by <a class="el" href="homkermap_8h.html">homkermap.h</a> can be used.</p>
<p>For additive kernels it is also possible to perform the feature expansion online inside the solver, setting the specific feature map via <a class="el" href="svmdataset_8h.html#a5c582cd8a267322745f8acdc2783e1d8" title="Set feature map.">vl_svmdataset_set_map</a>. This is particular useful to keep the size of the training data small, when the number of the samples is big or the memory is limited. </p>
</div></div><!-- contents -->
     <!-- Doc Here -->
    </div>
   </div>
   <div class="clear">&nbsp;</div>
  </div> <!-- pagebody -->
  <div id="footer">
   &copy; 2007-12 The VLFeat Authors
  </div> <!-- footer -->
 </body>
 <!-- Body ends -->
</html>
