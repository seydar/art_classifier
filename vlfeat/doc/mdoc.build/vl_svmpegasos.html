<group>
<ul class='breadcrumb'><li><a href='%pathto:mdoc;'>Index</a></li><li><a href='%pathto:vl_simdctrl;'>Prev</a></li><li><a href='%pathto:vl_twister;'>Next</a></li></ul><div class="documentation"><p>
W = <a href="%pathto:vl_svmpegasos;">VL_SVMPEGASOS</a>(DATA, LAMBDA) learns a linear SVM W given training
struct DATA, and the regularization parameter LAMBDA using the
PEGASOS [1] solver. The algorithm finds a minimizer W of the
objective function
</p><pre>
  LAMBDA/2 |W|^2 + 1/N SUM_i LOSS(W, X(:,i), Y(i))
</pre><p>
where LOSS(W,X,Y) = MAX(0, 1 - Y W'X) is the hinge loss and N is
the number of training vectors in X.
</p><p>
The training struct DATA is created using the function
VL_MAKETRAININGSET.
</p><p>
[W B INFO] = <a href="%pathto:vl_svmpegasos;">VL_SVMPEGASOS</a>(DATA, LAMBDA) learns a linear SVM W
and a bias B given training struct DATA, and the regularization
parameter LAMBDA using the PEGASOS [1] solver. INFO is a struct
containing the input parameters plus diagnostic informations:
</p><dl><dt>
energy
</dt><dd><p>
SVM energy value.
</p></dd><dt>
iterations
</dt><dd><p>
Number of iterations performed.
</p></dd><dt>
elapseTime
</dt><dd><p>
Elapsed time since the start of the SVM learning.
</p></dd><dt>
regulizerTerm
</dt><dd><p>
Value of the SVM regulizer term.
</p></dd><dt>
lossPos
</dt><dd><p>
Value of loss function only for data points labeled positives.
</p></dd><dt>
lossNeg
</dt><dd><p>
Value of loss function onlt for data points labeled negatives.
</p></dd><dt>
hardLossPos
</dt><dd><p>
Number of mislabeled positive points.
</p></dd><dt>
hardLossNeg
</dt><dd><p>
Number of mislabeled negative points.
</p></dd></dl><p>
ALGORITHM. PEGASOS is an implementation of stochastic subgradient
descent. At each iteration a data point is selected at random, the
subgradient of the cost function relative to that data point is
computed, and a step is taken in that direction. The step size is
inversely proportional to the iteration number. See [1] for
details.
</p><p>
<a href="%pathto:vl_svmpegasos;">VL_SVMPEGASOS</a>() accepts the following options:
</p><dl><dt>
Epsilon
<span class="defaults">[empty]</span></dt><dd><p>
Specify the SVM stopping criterion threshold. If not
specified VL_SVMPEGASOS will finish when the maximum number
of iterations is reached. The stopping criterion is tested
after each ENERGYFREQ iteration.
</p></dd><dt>
MaxIterations
<span class="defaults">[10 / LAMBDA]</span></dt><dd><p>
Sets the maximum number of iterations.
</p></dd><dt>
BiasMultiplier
<span class="defaults">[0]</span></dt><dd><p>
Appends to the data X the specified scalar value B. This
approximates the training of a linear SVM with bias.
</p></dd><dt>
StartingModel
<span class="defaults">[null vector]</span></dt><dd><p>
Specify the initial value for the weight vector W.
</p></dd><dt>
StartingIteration
<span class="defaults">[1]</span></dt><dd><p>
Specify the iteration number to start from. The only effect
is to change the step size, as this is inversely proportional
to the iteration number.
</p></dd><dt>
StartingBias
<span class="defaults">[0]</span></dt><dd><p>
Specify the inital bias value.
</p></dd><dt>
BiasLearningRate
<span class="defaults">[1]</span></dt><dd><p>
Specify the frequency of the bias learning. The default
setting updates the bias at each iteration.
</p></dd><dt>
Permutation
<span class="defaults">[empty]</span></dt><dd><p>
Specify a permutation PERM to be used to sample the data (this
disables random sampling). Specifically, at the T-th iteration
the algorithm takes a step w.r.t. the PERM[T']-th data point,
where T' is T modulo the number of data samples
(i.e. MOD(T'-1,NUMSAMPLES)+1). PERM needs not to be
bijective. This allows specifying certain data points more or
less frequently, implicitly increasing their relative weight in
the error term. A common application is to balance an unbalanced
dataset.
</p></dd><dt>
DiagnosticFunction
<span class="defaults">[empty]</span></dt><dd><p>
Specify a function handle to be called every ENERGYFREQ
iterations.
The function must be of the form:
</p><pre>
  function o = diagnostics(svm,x)
</pre><p>
Where in the first iteration x is the variable passed as
&quot;DiagnosticCallRef&quot;, and the consecutive ones x is the
variable o returned in the previous iteration.
</p></dd><dt>
DiagnosticCallRef
<span class="defaults">[empty]</span></dt><dd><p>
Specify a paramater to be passed to the DIAGNOSTICFUNCTION handle.
</p></dd><dt>
EnergyFreq
<span class="defaults">[100]</span></dt><dd><p>
Specify how often the SVM energy is computed.
</p></dd><dt>
ValidationData
</dt><dd><p>
Specify a validation dataset. The validation dataset must be
created using VL_MAKETRAININGSET. If specified, the energy
value and all the diagnostic informations are computed on the
validation dataset, otherwise the training dataset is used.
</p></dd><dt>
Verbose
</dt><dd><p>
Be verbose.
</p></dd><dt>
Example
</dt><dd><p>
The options StartingModel and StartingIteration can be used
to continue training. I.e., the command
</p><pre>
  vl_twister('state',0) ;
  dataset = vl_maketrainingset(x,y) ;
  w = vl_svmpegasos(dataset,lambda,'NumIterations',1000) ;
</pre><p>
produces the same result as the sequence
</p><pre>
  vl_twister('state',0) ;
  dataset = vl_maketrainingset(x,y) ;
  w = vl_svmpegasos(dataset,lambda,'NumIterations',500) ;
  w = vl_svmpegasos(dataset,lambda,'NumIterations',500, ...
                 'StartingIteration', 501, ...
                 'StartingModel', w) ;
</pre></dd><dt>
REFERENCES
</dt><dd><p>
[1] S. Shalev-Shwartz, Y. Singer, N. Srebro, and
A. Cotter. Pegasos: Primal Estimated sub-GrAdient SOlver for
SVM. MBP, 2010.
</p></dd></dl><p>
See also: <a href="%pathto:vl_maketrainingset;">VL_MAKETRAININGSET</a>(), <a href="%pathto:vl_homkermap;">VL_HOMKERMAP</a>(), <a href="%pathto:vl_help;">VL_HELP</a>().
</p></div></group>
